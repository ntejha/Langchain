{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GenAI App using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACKING_V2'] = \"True\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion -- from the wensite we need to scrape the data\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "docs = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/integrations/llms/\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Chunking \n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "final_document = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embeddings AND Vector storing\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "db = FAISS.from_documents(final_document,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you'd like to contribute an integration, see Contributing integrations.Features (natively supported)\\u200bAll LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below:Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread.Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations.Batch\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query from a vector store db\n",
    "\n",
    "query = \"Streaming support defaults\"\n",
    "\n",
    "results = db.similarity_search(query)\n",
    "\n",
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The context explains how to get a single value from an LLM provider using iterators. \\n\\n\\n**Here's a breakdown:**\\n\\n* **Iterator:**  A way to fetch values one at a time, similar to reading data line-by-line from a file.\\n* **LLM Provider:** The software you use (like ChatGPT) that actually processes the language input and generates text or data. \\n\\n\\n**Key points:**\\n\\n* **Single Value Output:** The final output of the LLM is usually a single value, not individual tokens like in real-time streaming.  \\n* **Iterator Support:**  This context emphasizes that using an iterator will work with any LLM provider that offers this kind of integration. \\n\\n\\nLet me know if you'd like more information about iterators or how they relate to the process of working with LLMs!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrival Chain, Document Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "llm = Ollama(model=\"gemma2:2b\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answe the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ") \n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\":\"Streaming support defaults\",\n",
    "    \"context\": [Document(page_content=\"to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations.\")]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we jsut set up. That way, we can use the retriver to dynamically to select the most relevant documents and pass those in for a given question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fe268893980>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='\\nAnswe the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'))])\n",
       "            | Ollama(model='gemma2:2b')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input --> Retriver --> vectorstoredb\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriver = db.as_retriever()\n",
    "retriver_chain = create_retrieval_chain(retriver,document_chain)\n",
    "\n",
    "retriver_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Streaming support defaults',\n",
       " 'context': [Document(metadata={'source': 'https://python.langchain.com/v0.2/docs/integrations/llms/', 'title': 'LLMs | ü¶úÔ∏èüîó LangChain', 'description': \"If you'd like to write your own LLM, see this how-to.\", 'language': 'en'}, page_content=\"If you'd like to contribute an integration, see Contributing integrations.Features (natively supported)\\u200bAll LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below:Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread.Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations.Batch\"),\n",
       "  Document(metadata={'source': 'https://python.langchain.com/v0.2/docs/integrations/llms/', 'title': 'LLMs | ü¶úÔ∏èüîó LangChain', 'description': \"If you'd like to write your own LLM, see this how-to.\", 'language': 'en'}, page_content='LLMs | ü¶úÔ∏èüîó LangChain'),\n",
       "  Document(metadata={'source': 'https://python.langchain.com/v0.2/docs/integrations/llms/', 'title': 'LLMs | ü¶úÔ∏èüîó LangChain', 'description': \"If you'd like to write your own LLM, see this how-to.\", 'language': 'en'}, page_content=\"doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations.Batch support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig.Each LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support.ModelInvokeAsync invokeStreamAsync streamBatchAsync\"),\n",
       "  Document(metadata={'source': 'https://python.langchain.com/v0.2/docs/integrations/llms/', 'title': 'LLMs | ü¶úÔ∏èüîó LangChain', 'description': \"If you'd like to write your own LLM, see this how-to.\", 'language': 'en'}, page_content='this page helpful?You can also leave detailed feedback on GitHub.PreviousZHIPU AINextLLMsFeatures (natively supported)CommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')],\n",
       " 'answer': \"The provided context explains how LLMs are integrated with LangChain and describes different types of integrations:\\n\\n* **Async, Streaming, Batch:** These features represent how a LLM performs requests (e.g., initiate a sequence of actions in background threads, process data in chunks, or execute continuously) based on specific requirements.\\n* **Default implementations**: Langchain provides basic support for async, streaming and batch using native methods from the underlying LLM provider. \\n* **Iterator:**  For streaming, it returns an iterator to allow token-by-token reading. While this doesn't enable full token-based streaming, it ensures compatibility across different LLMs.\\n* **Native implementations:** Some LLM providers offer more efficient implementation of async, streaming or batch for users that need them. \\n\\n\\nThis information helps understand the general mechanics and functionalities of interacting with LLMs through LangChain.  \\n\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response from the LLM\n",
    "\n",
    "response = retriver_chain.invoke({\"input\":\"Streaming support defaults\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
